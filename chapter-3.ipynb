{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Steps with Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt engineering involves crafting inputs to LLMs (prompts) that effectively communicate the task at hand to the LLM, leading it to return accurate and useful outputs (Figure 3.1). Prompt engineering is a skill that requires an understanding of the nuances of language, the specific domain being worked on, and the capabilities and limitations of the LLM being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing LLMs with Customized Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning hinges on the idea of transfer learning. Transfer learning is a technique that leverages pre-trained models to build upon existing knowledge for new tasks or domains. In the case of LLMs, this involves utilizing the pre-training to transfer general language understanding, including grammar and general knowledge, to particular domain-specific tasks. However, the pre-training may not be sufficient to understand the nuances of certain closed or specialized topics, such as a company’s legal structure or guidelines.\n",
    "\n",
    "### Fine-tuning is a specific form of transfer learning that adjusts the parameters of a pre-trained model to better suit a “downstream” target task. Through fine-tuning, LLMs can learn from custom examples and become more effective at generating relevant and accurate responses.\n",
    "\n",
    "\n",
    "### Training set: A collection of labeled examples used to train the model. The model learns to recognize patterns and relationships in the data by adjusting its parameters based on the training examples.\n",
    "\n",
    "### Validation set: A separate collection of labeled examples used to evaluate the model’s performance during training.\n",
    "\n",
    "### Test set: A third collection of labeled examples that is separate from both the training and validation sets. It is used to evaluate the final performance of the model after the training and fine-tuning processes are complete. The test set provides a final, unbiased estimate of the model’s ability to generalize to new, unseen data.\n",
    "\n",
    "### Loss function: A function that quantifies the difference between the model’s predictions and the actual target values. It serves as a metric of error to evaluate the model’s performance and guide the optimization process. During training, the goal is to minimize the loss function to achieve better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The process of fine-tuning can be broken down into a few steps:\n",
    "\n",
    "### Collecting labeled data: The first step in fine-tuning is to gather our training, validation, and testing datasets of labeled examples relevant to the target task or domain. Labeled data serves as a guide for the model to learn the task-specific patterns and relationships. For example, if the goal is to fine-tune a model for sentiment classification (our first example), the dataset should contain text examples along with their respective sentiment labels, such as positive, negative, or neutral.\n",
    "\n",
    "### Hyperparameter selection: Fine-tuning involves adjusting hyperparameters that influence the learning process—for example, the learning rate, batch size, and number of epochs. The learning rate determines the step size of the model’s weight updates, while the batch size refers to the number of training examples used in a single update. The number of epochs denotes how many times the model will iterate over the entire training dataset. Properly setting these hyperparameters can significantly impact the model’s performance and help prevent issues such as overfitting (i.e., when a model learns the noise in the training data more than the signals) and underfitting (i.e., when a model fails to capture the underlying structure of the data).\n",
    "\n",
    "### Model adaptation: Once the labeled data and hyperparameters are set, the model may have to be adapted to the target task. This involves modifying the model’s architecture, such as adding custom layers or changing the output structure, to better suit the target task. For example, BERT’s architecture cannot perform sequence classification as is, but it can be modified very slightly to carry out this task.\n",
    "\n",
    "### Evaluation and iteration: After the fine-tuning process is complete, we have to evaluate the model’s performance on a separate holdout validation set to ensure that it generalizes well to unseen data. Performance metrics such as accuracy, F1 score, or mean absolute error (MAE) can be used for this purpose, depending on the task. If the performance is not satisfactory, adjustments to the hyperparameters or dataset may be necessary, followed by retraining the model.\n",
    "\n",
    "### Model implementation and further training: Once the model is fine-tuned and we are happy with its performance, we need to integrate it with existing infrastructures in a way that can handle any errors and collect feedback from users. Doing so will enable us to add to our total dataset and rerun the process in the future.\n",
    "\n",
    "## Closed-Source Pre-trained Models as a Foundation\n",
    "### Pre-trained LLMs play a vital role in transfer learning and fine-tuning, providing a foundation of general language understanding and knowledge. This foundation allows for efficient adaptation of the models to specific tasks and domains, reducing the need for extensive training resources and data.\n",
    "\n",
    "### This chapter focuses on fine-tuning LLMs using OpenAI’s infrastructure, which has been specifically designed to facilitate this process. OpenAI has developed tools and resources to make it easier for researchers and developers to fine-tune smaller models, such as Ada and Babbage, for their specific needs. The infrastructure offers a streamlined approach to fine-tuning, allowing users to efficiently adapt pre-trained models to a wide variety of tasks and domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning rate: The learning rate determines the size of the steps the model takes during optimization. A smaller learning rate leads to slower convergence but potentially better accuracy, while a larger learning rate speeds up training but may cause the model to overshoot the optimal solution.\n",
    "\n",
    "- Batch size: Batch size refers to the number of training examples used in a single iteration of model updates. A larger batch size can lead to more stable gradients and faster training, while a smaller batch size may result in a more accurate model but slower convergence.\n",
    "\n",
    "- Training epochs: An epoch is a complete pass through the entire training dataset. The number of training epochs determines how many times the model will iterate over the data, allowing it to learn and refine its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Learning\n",
    "\n",
    "### This process of taking smaller steps in training and updating already fine-tuned models for more training steps/epochs with new labeled datapoints is called incremental learning, also known as continuous learning or online learning. Incremental learning often results in more controlled learning, which can be ideal when working with smaller datasets or when you want to preserve some of the model’s general knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Prompting\n",
    "\n",
    "### Batch prompting allows LLMs to run inferences in batches, instead of one sample at a time, as we did with our fine-tuned ADA model from Chapter 4. This technique significantly reduces both token and time costs while maintaining or, in some cases, improving performance in various tasks.\n",
    "\n",
    "### The concept behind batch prompting is to group multiple samples into a single prompt so that the LLM generates multiple responses simultaneously. This process reduces the LLM inference time from N to roughly N/b, where b is the number of samples in a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Chaining\n",
    "### Prompt chaining involves using one LLM output as the input to another LLM so as to complete a more complex or multistep task. This can be a powerful way to leverage the capabilities of multiple LLMs and to achieve results that would not be possible with a single model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By breaking up complex tasks into smaller, more manageable prompts, we can often achieve the following benefits:\n",
    "\n",
    "- Specialization: Each LLM in the chain can focus on its area of expertise, allowing for more accurate and relevant results in the overall solution.\n",
    "\n",
    "- Flexibility: The modular nature of chaining allows for the easy addition, removal, or replacement of LLMs in the chain to adapt the system to new tasks or requirements.\n",
    "\n",
    "- Efficiency: Chaining LLMs can lead to more efficient processing, as each LLM can be fine-tuned to address its specific part of the task, reducing the overall computational cost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
