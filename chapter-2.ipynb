{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text embeddings\n",
    "### Text embeddings are a way to represent words or phrases as machine-readable numerical vectors in a multidimensional space, generally based on their contextual meaning. The idea is that if two phrases are similar (we will explore the word “similar” in more detail later on in this chapter), then the vectors that represent those phrases should be close together by some measure (like Euclidean distance), and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymmetric Semantic Search\n",
    "\n",
    "### A semantic search system can understand the meaning and context of your search query and match it against the meaning and context of the documents that are available to retrieve. This kind of system can find relevant results in a database without having to rely on exact keyword or n-gram matching; instead, it relies on a pre-trained LLM to understand the nuances of the query and the documents \n",
    "\n",
    "### The asymmetric part of asymmetric semantic search refers to the fact that there is an imbalance between the semantic information (basically the size) of the input query and the documents/information that the search system has to retrieve. Basically, one of them is much shorter than the other. For example, a search system trying to match “magic the gathering cards” to lengthy paragraphs of item descriptions on a marketplace would be considered asymmetric. The four-word search query has much less information than the paragraphs but nonetheless is what we have to compare.\n",
    "\n",
    "### Asymmetric semantic search systems can produce very accurate and relevant search results, even if you don’t use exactly the right words in your search. They rely on the learnings of LLMs rather than the user being able to know exactly which needle to search for in the haystack.\n",
    "\n",
    "They can be overly sensitive to small variations in text, such as differences in capitalization or punctuation.\n",
    "\n",
    "They struggle with nuanced concepts, such as sarcasm or irony, that rely on localized cultural knowledge.\n",
    "\n",
    "They can be more computationally expensive to implement and maintain than the traditional method, especially when launching a home-grown system with many open-source components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedder\n",
    "\n",
    "### At the heart of any semantic search system is the text embedder. This component takes in a text document, or a single word or phrase, and converts it into a vector. The vector is unique to that text and should capture the contextual meaning of the phrase.\n",
    "\n",
    "### The choice of the text embedder is critical, as it determines the quality of the vector representation of the text. We have many options for how we vectorize with LLMs, both open and closed source. To get off of the ground more quickly, we will use OpenAI’s closed-source “Embeddings” product for our purposes here. In a later section, I’ll go over some open-source options.\n",
    "\n",
    "### OpenAI’s “Embeddings” is a powerful tool that can quickly provide high-quality vectors, but it is a closed-source product, which means we have limited control over its implementation and potential biases. In particular, when using closed-source products, we may not have access to the underlying algorithms, which can make it difficult to troubleshoot any issues that arise.\n",
    "\n",
    "## What Makes Pieces of Text “Similar”\n",
    "\n",
    "### Once we convert our text into vectors, we have to find a mathematical representation of figuring out whether pieces of text are “similar.” Cosine similarity is a way to measure how similar two things are. It looks at the angle between two vectors and gives a score based on how close they are in direction. If the vectors point in exactly the same direction, the cosine similarity is 1. If they’re perpendicular (90 degrees apart), it’s 0. And if they point in opposite directions, it’s –1. The size of the vectors doesn’t matter; only their orientation does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the OpenAI API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai.embeddings_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the necessary modules for the script to run\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_embedding\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Setting the engine to be used for text embedding\u001b[39;00m\n\u001b[1;32m      6\u001b[0m ENGINE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-embedding-ada-002\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai.embeddings_utils'"
     ]
    }
   ],
   "source": [
    "# Importing the necessary modules for the script to run\n",
    "import openai\n",
    "from openai.embeddings_utils import get_embedding\n",
    "\n",
    "# Setting the engine to be used for text embedding\n",
    "ENGINE = 'text-embedding-ada-002'\n",
    "\n",
    "# Generating the vector representation of the given text using the specified engine\n",
    "embedded_text = get_embedding('I love to be vectorized', engine=ENGINE)\n",
    "\n",
    "# Checking the length of the resulting vector to ensure it is the expected size (1536)\n",
    "len(embedded_text) == '1536'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d742a289e0e42f49644cf4806c920fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bed3beba00749aa9379232e3ad77a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f83fe07aef54ac9804f7824425f0775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3657c503192042cd8da52227e862f45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82606bd7235b41daa4eaad19f7ba03b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75403035b674b04bc765968fbbbf1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c26e180cda4f928d59085bf0956ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96b6e35894948f4b526e83caf9b8f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0cfe9355554187ac644bee1101a9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386f2300f11b4ef89e4929b85b26166a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d04c4e9bc3249b8b9b3bd0f81d4b047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885465fd1fba49cf8f7862a9e4dfd537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the SentenceTransformer library\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initializing a SentenceTransformer model with the 'multi-qa-mpnet-base-cos-v1'\n",
    "model = SentenceTransformer(\n",
    "  'sentence-transformers/multi-qa-mpnet-base-cos-v1')\n",
    "\n",
    "# Defining a list of documents to generate embeddings for\n",
    "docs = [\n",
    "          \"Around 9 million people live in London\",\n",
    "          \"London is known for its financial district\"\n",
    "       ]\n",
    "\n",
    "# Generate vector embeddings for the documents\n",
    "doc_emb = model.encode(\n",
    "    docs,                   # Our documents (an iterable of strings)\n",
    "    batch_size=32,          # Batch the embeddings by this size\n",
    "    show_progress_bar=True  # Display a progress bar\n",
    ")\n",
    "\n",
    "# The shape of the embeddings is (2, 768), indicating a length of 768 and two\n",
    "doc_emb.shape  #  == (2, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/pds2.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Open the PDF file in read-binary mode\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/pds2.pdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Create a PDF reader object\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     reader \u001b[38;5;241m=\u001b[39m PyPDF2\u001b[38;5;241m.\u001b[39mPdfReader(file)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Initialize an empty string to hold the text\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/machine-learning-env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/pds2.pdf'"
     ]
    }
   ],
   "source": [
    "# Use the PyPDF2 library to read a PDF file\n",
    "import PyPDF2\n",
    "import tqdm\n",
    "\n",
    "# Open the PDF file in read-binary mode\n",
    "with open('../data/pds2.pdf', 'rb') as file:\n",
    "\n",
    "    # Create a PDF reader object\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Initialize an empty string to hold the text\n",
    "    principles_of_ds = ''\n",
    "\n",
    "    # Loop through each page in the PDF file\n",
    "    for page in tqdm(reader.pages):\n",
    "\n",
    "        # Extract the text from the page\n",
    "        text = page.extract_text()\n",
    "        # Find the starting point of the text we want to extract\n",
    "        # In this case, we are extracting text starting from the string ' ]'\n",
    "        principles_of_ds += '\\n\\n' + text[text.find(' ]')+2:]\n",
    "\n",
    "# Strip any leading or trailing whitespace from the resulting string\n",
    "principles_of_ds = principles_of_ds.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
